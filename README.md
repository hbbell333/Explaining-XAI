# Explaining-XAI
AIPI 590 Explaining XAI Assignment

This meme attempts to explain the paper [THE UNLOCKING SPELL ON BASE LLMS: RETHINKING ALIGNMENT VIA IN-CONTEXT LEARNING](https://arxiv.org/pdf/2312.01552). 

This paper investigates the possibility that AI alignment may be largely "superficial", and suggests an alternative method for alignment through in-context learning without any fine-tuning procedures. The authors find that aligned models primarily shift from base models in their use of stylistic tokens, while largely agreeing on the rest of response generations (when conditioned on the aligned model's partial outputs). They propose an in-context learning alignment method that uses high-quality, stylized, static prompts (both system and in-context) to align LLM outputs.

My meme explains a few of the findings of this paper. It builds on the "two-paths" meme, which depicts a person at a crossroads, where one path leads to a sunny and welcoming castle, and the other leads to a gloomy and haunting one. This is not dissimilar from how the authors explain alignment in this paper. Their findings suggest that alignment hinges on stylistic tokens, such as 'Hello', 'Thank', or 'However'. They show that aside from these tokens, both the aligned and unaligned models show high levels of agreement in how to proceed with response generation. This suggests that seemingly innocuous decision points, like whether or not to greet the user before generating a response, can lead the model to either the sunny castle or the gloomy one. 

I expanded the original meme to have multiple branching paths of these stylistic decision points to show that simply forcing a model to start each response with "Hello" is not sufficient for alignment. While the models in the paper primarily differed in their use of stylistic language, it is not trivial to know when to use such language. The multiple layers of decision points highlight that if a model fails at any point to insert an appropriate stylistic token, it can easily end up out of alignment.

Lastly, it's important to note that many of the phrases that send a model down the unaligned path are not themselves particularly offensive. To me, the results of this paper suggest that 'good' stylistic language, due to the autoregressive nature of LLM generation, can push a model into producing more aligned responses. While the unaligned model may not have used the same stylistic tokens as the aligned model, my suspicion is that it would often not immediately resort to unaligned behaviors. The authors suggest that the stylistic tokens used by the aligned model serve as important markers to **steer future token generation** away from potentially harmful responses, and toward well-aligned ones. In this way the stylistic tokens serve as a prophylactic, reducing the probability of generating tokens that conflict with the well-aligned attributes of whatever stylistic was used. 

!(https://github.com/hbbell333/Explaining-XAI/blob/main/Unlocking_Spell_meme.png)
